# Домашнее задание к занятию «Микросервисы: подходы»

Вы работаете в крупной компании, которая строит систему на основе микросервисной архитектуры.
Вам как DevOps-специалисту необходимо выдвинуть предложение по организации инфраструктуры для разработки и эксплуатации.


## Задача 1: Обеспечить разработку

Предложите решение для обеспечения процесса разработки: хранение исходного кода, непрерывная интеграция и непрерывная поставка. 
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- облачная система;
- система контроля версий Git;
- репозиторий на каждый сервис;
- запуск сборки по событию из системы контроля версий;
- запуск сборки по кнопке с указанием параметров;
- возможность привязать настройки к каждой сборке;
- возможность создания шаблонов для различных конфигураций сборок;
- возможность безопасного хранения секретных данных (пароли, ключи доступа);
- несколько конфигураций для сборки из одного репозитория;
- кастомные шаги при сборке;
- собственные докер-образы для сборки проектов;
- возможность развернуть агентов сборки на собственных серверах;
- возможность параллельного запуска нескольких сборок;
- возможность параллельного запуска тестов.

Обоснуйте свой выбор.

### Ответ

Для обеспечения процесса разработки с учетом всех перечисленных требований стоит рассмотреть GitLab + HashiCorp Vault для хранения секретов.

GitLab - предоставляет полностью интегрированное решение "все-в-одном" (SCM + CI/CD + Container Registry) из коробки

**Обоснование выбора и соответствие требованиям**

Требование	|	Реализация в предложенном решении (GitLab CI/CD + Vault)
-|-
Облачная система	|	GitLab.com (SaaS) или Self-Managed вариант в собственном облаке (AWS, GCP, Azure). Оба варианта являются "облачными".
Система контроля версий Git	|	GitLab — это Git-хостинг с полноценной веб-UI оболочкой, MR (Merge Requests), код-ревью и т.д.
Репозиторий на каждый сервис	|	Стандартная практика GitLab. Каждый микросервис — это отдельный проект (репозиторий) в группе services.
Запуск сборки по событию из СКВ	|	Стандартная функция. Пайплайн запускается на событиях: push, merge_request, tag.
Запуск сборки по кнопке с параметрами	|	Manual Jobs + Pipeline Parameters. Можно описать job с тегом when: manual, который будет ждать нажатия кнопки. С помощью клюwords workflow:rules или rules можно запускать весь пайплайн с параметрами (например, VERSION).
Настройки к каждой сборке	|	Переменные CI/CD (CI/CD Variables). Можно задать переменные на уровне проекта, группы или всей инстанции. Они безопасно передаются в среду выполнения пайплайна.
Создание шаблонов для конфигураций	|	1. include:template (встроенные шаблоны GitLab). <br> 2. include:file / include:remote — вынесение общих шагов в отдельный .gitlab-ci.yml файл в другом репозитории и его reuse. <br> 3. extends — наследование и переопределение job.
Безопасное хранение секретов	|	1. Masked & Protected Variables (встроено в GitLab). Переменные можно пометить как masked (не показываются в логах) и protected (доступны только на защищенных ветках). <br> 2. Интеграция с HashiCorp Vault. Нативный плагин позволяет динамически получать short-lived секреты из Vault прямо в job. Это лучшая практика.
Несколько конфигураций из одного репозитория	|	1. workflow:rules и rules в каждом job. Позволяют по условию (ветка, тег, источник, параметр) решать, запускать ли job или весь пайплайн. <br> Пример: Разные этапы для dev, stage, prod из одного файла конфигурации.
Кастомные шаги при сборке	|	Любые. .gitlab-ci.yml описывает шаги в виде скриптов (shell, PowerShell). Можно выполнить любую команду, написать собственный скрипт на любом языке.
Собственные докер-образы для сборки	|	Ключевое слово image. Для каждого job можно указать свой Docker-образ, в котором будет запускаться сборка. Эти образы хранятся в GitLab Container Registry, что обеспечивает быстрый доступ и безопасность.
Развертывание агентов на своих серверах	|	GitLab Runner. Легко устанавливается на любой сервер (Linux, Windows, macOS) или в Kubernetes. Регистрируется в GitLab и выполняет задачи. Можно иметь сколько угодно раннеров с разными тегами (например, docker, kubernetes, windows).
Параллельный запуск сборок	|	Неограниченно. Каждый GitLab Runner может обрабатывать несколько job параллельно (настраивается). Можно иметь пул из dozens раннеров. Каждая сборка (pipeline) и каждый job внутри нее по умолчанию независимы.
Параллельный запуск тестов	|	Ключевое слово parallel. Позволяет запустить один job (например, run-tests) в N параллельных копиях. GitLab автоматически разделит тесты между ними (например, с помощью CI_NODE_INDEX), что drastically сокращает время прохождения тестовой стадии.

**Преимущества данного стека**

- Единая платформа (Single Pane of Glass): Разработчики не переключаются между интерфейсами GitHub и отдельным Jenkins. Всё (код, MR, CI/CD, артефакты) находится в одном месте — GitLab.

 - Инфраструктура как код (IaC): Конфигурация CI/CD полностью описывается в файле .gitlab-ci.yml, который хранится в репозитории и версионируется вместе с кодом.

 - Масштабируемость: Runner'ы, развернутые в Kubernetes, могут автоматически масштабироваться с помощью cluster-autoscaler под нагрузку.

 - Безопасность: Интеграция с Vault вместо хранения всех секретов внутри GitLab обеспечивает профессиональный уровень безопасности, аудита и ротации секретов.

- Сообщество и корпоративная поддержка: GitLab Inc. предоставляет отличную документацию и коммерческую поддержку для Enterprise-клиентов.

В качестве альтернативы можно расмотреть Git + Хостинг репозиториев + Jenkins

Критерий	|	GitLab (All-in-One Platform)	|	GitHub + Jenkins (Best-of-Breed Combo)
-|-|-
Архитектура и Общая идея	|	Единая интегрированная платформа для всего SDLC: SCM, CI/CD, Issue Tracking, Wiki, Container Registry в одном продукте.	|	Связка лучших инструментов: GitHub — лучший Git-хостинг и соц. сеть для разработчиков. Jenkins — самый гибкий и мощный движок CI/CD.
Соответствие требованиям	|	~100%. Покрывает все требования нативно.	|	~100%. Покрывает все требования за счет гибкости Jenkins и экосистемы плагинов.
Сила решения	|	Интеграция и "единый источник истины". Скорость работы, простота настройки для стандартных сценариев. Все находится в одном месте.	|	Максимальная гибкость и мощь. Jenkins можно заставить делать literally что угодно. Огромнейшее сообщество и арсенал плагинов.
Простота настройки и обучения	|	✅ Высокая. Единственный инструмент, один файл конфигурации (.gitlab-ci.yml), единая модель прав доступа.	|	🔴 Низкая. Требует глубоких знаний и администрирования двух сложных систем и их интеграции.
Простота эксплуатации и поддержки	|	✅ Высокая (для SaaS). Средняя (для Self-Hosted). Один продукт для обновления и мониторинга.	|	🔴 Очень низкая. Jenkins требует постоянного обслуживания: обновления, борьба с плагинами, безопасность, мониторинг.
Производительность и масштабируемость	|	✅ Хорошая. GitLab Runner легковесный и легко масштабируется в Kubernetes.	|	✅ Отличная. Jenkins Master/Agent архитектура хорошо масштабируется. Агенты (агенты) можно развернуть где угодно.
Безопасное хранение секретов	|	✅✅ Отлично. Masked Variables + нативная интеграция с HashiCorp Vault.	|	🟡 Зависит от плагинов. Есть плагины для Vault, Credentials Plugin, но это дополнительный уровень настройки.
Создание шаблонов	|	✅ Хорошо. include:remote, extends, общие конфиги.	|	✅✅ Превосходно. Shared Libraries в Jenkins — это мощнейший инструмент для вынесения всей логики пайплайнов в отдельные репозитории.
Визуализация и отчетность	|	✅ Очень хорошо. Интеграция пайплайнов в Merge Request, артефакты, отчеты о тестах — все в одном интерфейсе.	|	🟡 Разрозненно. Статус сборки в GitHub — это лишь ссылка на Jenkins. Для просмотра логов и артефактов нужно уходить в Jenkins UI.
Вендор-локин	|	🟡 Умеренный. Формат .gitlab-ci.yml — это YAML, который можно с определенными усилиями конвертировать.	|	✅ Минимальный. GitHub — де-факто стандарт для хостинга кода. Jenkins Job DSL или Declarative Pipeline — это гибкий код, который можно адаптировать под другие системы.
Сообщество и плагины	|	✅ Большое и активное сообщество.	|	✅✅ Огромнейшее. Jenkins обладает самой большой экосистемой плагинов на рынке CI/CD.
Стоимость владения (TCO)	|	✅ Предсказуемая. Плата за лицензию (если EE) и/или инфраструктуру. Низкие операционные затраты.	|	🔴 Непредсказуемая. Бесплатный софт, но очень высокие операционные (операционные) затраты на администрирование и поддержку Jenkins.

**Преимущество GitLab** - это инстумент "Все в одном" не нужно переключатся между инструментами, более простое развертывание, настройка, эксплуатация поддержка.

**Преимущество Git + Jenkins** - более гибкое и мощное решение, обладает самой большой экосистемой плагинов на рынке CI/CD. Бесплатное решение.



## Задача 2: Логи

Предложите решение для обеспечения сбора и анализа логов сервисов в микросервисной архитектуре.
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- сбор логов в центральное хранилище со всех хостов, обслуживающих систему;
- минимальные требования к приложениям, сбор логов из stdout;
- гарантированная доставка логов до центрального хранилища;
- обеспечение поиска и фильтрации по записям логов;
- обеспечение пользовательского интерфейса с возможностью предоставления доступа разработчикам для поиска по записям логов;
- возможность дать ссылку на сохранённый поиск по записям логов.

Обоснуйте свой выбор.

### Ответ

Для сбора логов стоит расмотреть связку Vector + ClickHouse + Grafana
1. Vector (в режиме DaemonSet в Kubernetes) собирает логи из /var/log/containers/*.log.

2. Vector парсит JSON, добавляет метки (метаданные Pod, ноды и т.д.), буферизует данные и надежно отправляет их напрямую в ClickHouse.

3. ClickHouse принимает и хранит логи в оптимально сжатом виде. Создается таблица с колонками для всех основных полей лога (service, level, message, request_id и т.д.).

4. Grafana подключается к ClickHouse как к источнику данных. Разработчики используют встроенный SQL-редактор Grafana для написания запросов.

    - Пример простого поиска: SELECT * FROM logs WHERE service='auth-service' AND level='ERROR' ORDER BY timestamp DESC LIMIT 100

    - Пример аналитики: SELECT service, count() as error_count FROM logs WHERE level='ERROR' AND timestamp > now() - INTERVAL 1 HOUR GROUP BY service ORDER BY error_count DESC

5. Любой запрос в Grafana можно сохранить и получить на него прямую ссылку для общего доступа.


**В качестве альтернативы рассмотрим классическое решение Стек EFK**

Критерий	|	EFK/ECK <br>(Fluent Bit -> Kafka -> ES -> Kibana)	|	Vector + ClickHouse + Grafana <br>(Vector -> ClickHouse -> Grafana)
----|-----|-
Архитектура и философия	|	Проверенная временем. Раздельные best-of-breed tools, соединенные вместе.	|	Высокопроизводительная и экономичная. Vector — универсальный агент, ClickHouse — мощнейшее колоночное хранилище.
Соответствие требованиям	|	100%	|	100%
Агент сбора (на ноде)	|	Fluent Bit<br>Легковесный, надежный.	|	Vector<br>Ключевое преимущество: Крайне высокая производительность и низкое потребление ресурсов (написано на Rust).<br>Встроенная буферизация и возможность трансформации данных (парсинг, обогащение) до отправки.
Буферизация (гарантированная доставка)	|	Требуется Kafka (или аналоги). Добавляет операционную сложность.	|	Встроенная в Vector. Vector может буферизовать данные на диске локально и управлять повторами. Может отправлять в Kafka (если нужно), но не требует этого.
Хранилище	|	Elasticsearch<br>· Дорогой в эксплуатации (высокие требования к RAM и CPU).<br>· Сложность управления кластером (shards, replicas).<br>· Полнотекстовый поиск — сильная сторона.	|	ClickHouse<br>· Ключевое преимущество: Невероятно экономичное хранилище. Высокая скорость записи и сжатие данных (в 5-10 раз лучше ES).<br>· Идеально подходит для структурированных/полуструктурированных логов (JSON).<br>· Молниеносные агрегации и GROUP BY.
Поиск и фильтрация	|	Kibana + KQL<br>· Идеален для ad-hoc полнотекстового поиска.<br>· Интуитивный интерфейс для "гуманитариев".	|	Grafana + SQL<br>· Поиск через SQL-запросы к ClickHouse.<br>· Сильная сторона: Не поиск единичного лога, а анализ больших объемов: "посчитай количество ошибок по каждому сервису за последний час".<br>· Меньше приспособлен для случайного "гугления" логов.
Пользовательский интерфейс	|	Kibana<br>· Специализированный инструмент для логов.<br>· Saved Search, дашборды.	|	Grafana<br>· Ключевое преимущество: Единый стек для логов (Loki/ClickHouse), метрик (Prometheus) и трейсов (Tempo).<br>· Возможность создавать дашборды, коррелирующие метрики и логи (например, график latency + примеры медленных запросов из логов).
Ссылка на сохраненный поиск	|	✅ Естественно поддерживается в Kibana.	|	✅ Поддерживается в Grafana. Можно сохранить запрос с панели и получить прямую ссылку.
Стоимость владения (TCO)	|	Высокая<br>· Большой кластер Elasticsearch требует мощного "железа" и экспертизы для настройки и поддержки.	|	Низкая<br>· ClickHouse требует значительно меньше ресурсов для хранения того же объема данных.<br>· Vector эффективнее Fluent Bit.<br>· Меньше компонентов (не нужна отдельная Kafka для буфера).
Операционная сложность	|	Высокая<br>· Необходимо администрировать и масштабировать 3 сложных системы: Kafka, ES, Kibana.	|	Средняя<br>· ClickHouse проще в эксплуатации, чем ES.<br>· Vector проще и надежнее Fluent Bit/Fluentd.

**Детальный анализ и итоговый вывод**
Оба стека полностью соответствуют требованиям. Однако их внутренняя философия и сильные стороны различны.

**Стек EFK (Elasticsearch) идеален, если:**

- Основная задача — это интерактивный, ад-hoc поиск по логам, аналогичный поиску в Google.

- Есть команда с экспертизой по поддержке Elasticsearch и Kafka.

- Нет ограничений в бюджете на инфраструктуру.

**Стек Vector + ClickHouse + Grafana идеален, если:**

- Производительность и экономичность — ключевые факторы. Имеет возможность обрабатывать огромные объемы данных (десятки-сотни ТБ логов) с минимальными затратами.

- Нужна корреляция логов с метриками и трейсами в едином интерфейсе (Grafana).

- Нужны не столько точечные поиски, сколько аналитические запросы к логам: "агрегируй, посчитай, отсортируй".

- Нужно упростить архитектуру, убрав из нее Kafka (за счет встроенной буферизации в Vector).

**Итог**
Для микросервисной архитектуры, где логи — это в первую очередь структурированные события (JSON), и где важна общая observability (метрики + логи + трейсы), связка Vector + ClickHouse + Grafana является более современным, производительным и экономичным выбором.

## Задача 3: Мониторинг

Предложите решение для обеспечения сбора и анализа состояния хостов и сервисов в микросервисной архитектуре.
Решение может состоять из одного или нескольких программных продуктов и должно описывать способы и принципы их взаимодействия.

Решение должно соответствовать следующим требованиям:
- сбор метрик со всех хостов, обслуживающих систему;
- сбор метрик состояния ресурсов хостов: CPU, RAM, HDD, Network;
- сбор метрик потребляемых ресурсов для каждого сервиса: CPU, RAM, HDD, Network;
- сбор метрик, специфичных для каждого сервиса;
- пользовательский интерфейс с возможностью делать запросы и агрегировать информацию;
- пользовательский интерфейс с возможностью настраивать различные панели для отслеживания состояния системы.

Обоснуйте свой выбор.

### Ответ

Предлагаемое решение: Стек Prometheus + VictoriaMetrics + Grafana
Для сбора и анализа метрик в микросервисной архитектуре стандартом является стек на основе Prometheus.

Я предлагаю использовать следующую комбинацию:

- Сбор метрик: Prometheus (+ экспортеры)

- Долгосрочное хранение метрик (опционально, но рекомендуется): VictoriaMetrics (или Thanos/Cortex)

- Визуализация и дашборды: Grafana

- Агенты на хостах: Node Exporter (для метрик хоста) и cAdvisor (для метрик контейнеров)

Детальное описание компонентов и их взаимодействие
1. Сбор метрик (Prometheus и Экспортеры)

**Принцип работы:** Prometheus использует модель pull. Это означает, что центральный сервер Prometheus периодически сам опрашивает (scrape) целевые endpoints, которые предоставляют метрики в определенном текстовом формате.

- Для метрик хостов (CPU, RAM, Disk, Network):
    - На каждой физической или виртуальной машине запускается легковесный Node Exporter.
    - Node Exporter предоставляет по HTTP endpoint (http://node-ip:9100/metrics) огромное количество метрик о состоянии хоста.
    - Prometheus раз в ~15-30 секунд опрашивает этот endpoint и забирает метрики.
- Для метрик контейнеров и потребления ресурсов сервисами (CPU, RAM на контейнер):
    - На каждой ноде в Kubernetes запускается cAdvisor (часто уже встроен в kubelet).
    - cAdvisor автоматически обнаруживает все контейнеры на ноде и предоставляет детальные метрики по использованию CPU, памяти, сети и диска для каждого контейнера.
    - Prometheus опрашивает cAdvisor на каждой ноде.
- Для специфичных метрик каждого сервиса (бизнес-метрики):
    - Для многих языков существуют библиотеки(Prometheus Client для Python, Go, Java и т.д.), для вывода метрик (например, http_requests_total, orders_processed, database_query_duration_seconds).
    - Приложение (микросервис) предоставлятет endpoint (например, /metrics).
    - Prometheus опрашивает этот endpoint и забирает кастомные метрики.
- Обнаружение целей (Service Discovery):
    - В Kubernetes Prometheus автоматически находит все цели для сбора метрик через Kubernetes Service Discovery. Он узнает о всех Pod'ах, Service'ах и нодах из API Kubernetes и динамически добавляет их в список для опроса.

2. Хранение метрик (Prometheus + VictoriaMetrics)
- Prometheus сам по себе хранит данные на диске локально. Но это не долгосрочное решение.
- Для промышленной эксплуатации рекомендуется использовать решение для долгосрочного, надежного и экономичного хранения. VictoriaMetrics — это современный, производительный и простой в эксплуатации fork Prometheus, который идеально подходит для этой роли.

- Принцип: Prometheus может настроен на запись всех собранных данных не только локально, но и удаленно (remote write) в VictoriaMetrics. VictoriaMetrics становится центральным хранилищем для всех данных.

3. Визуализация и анализ (Grafana)
- Grafana подключается к VictoriaMetrics (или напрямую к Prometheus) как к источнику данных.

- Пользовательский интерфейс для запросов: В Grafana используется мощный язык запросов PromQL (Prometheus Query Language). С его помощью можно делать любые агрегации, joins, математические операции над метриками.

- Пользовательский интерфейс с панелями: В Grafana можно создавать дашборды, состоящие из множества панелей (графики, таблицы, heatmaps, alert states). Эти дашборды можно настраивать под нужды каждой команды (дашборд для БД, дашборд для бизнес-метрик, дашборд общего состояния кластера).

**Соответствие требованиям**
Требование	|	Реализация в предложенном стеке (Prometheus + VictoriaMetrics + Grafana)
-|-
Сбор метрик со всех хостов	|	✅ Node Exporter на каждом хосте + Prometheus с Service Discovery.
Сбор метрик состояния ресурсов хостов	|	✅ Node Exporter предоставляет метрики: node_cpu_seconds_total, node_memory_MemAvailable_bytes, node_disk_io_time_seconds_total, node_network_receive_bytes_total.
Сбор метрик потребляемых ресурсов для каждого сервиса	|	✅ cAdvisor предоставляет метрики: container_cpu_usage_seconds_total, container_memory_usage_bytes, container_network_receive_bytes_total с labels pod, container, namespace.
Сбор метрик, специфичных для каждого сервиса	|	✅ Разработчики добавляют в код метрики с помощью Prometheus Client Libraries. Endpoint /metrics опрашивается Prometheus.
Пользовательский интерфейс с запросами и агрегацией	|	✅ Grafana с языком запросов PromQL позволяет делать любые агрегации (sum(), rate(), avg(), quantile()).
Пользовательский интерфейс с настраиваемыми панелями	|	✅ Grafana — мировой лидер для создания дашбордов. Панели настраиваются через UI или код (JSON).

**Обоснование выбора**
- Фактический стандарт: Prometheus — основа Cloud Native мониторинга. Его поддерживают все крупные провайдеры и Kubernetes.

- Модель Pull: Более безопасна и проста в настройке, чем push (не нужно открывать входящие порты на сервисах).

- Мощный язык запросов PromQL: Позволяет отвечать на самые сложные вопросы о работе системы.

- Экосистема: Огромное количество готовых экспортеров (для БД, очередей, hardware) и готовых дашборд для Grafana.

- Экономичность: VictoriaMetrics хранит данные с высочайшей степенью сжатия, что значительно дешевле альтернатив.

- Надежность и простота: Стек очень устойчив и хорошо предсказуем в работе.

